{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 2\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "Your hometown mayor just created a new data analysis team to give policy advice, and the administration recruited _you_ via LinkedIn to join it. Unfortunately, due to budget constraints, for now the \"team\" is just you...\n",
    "\n",
    "The mayor wants to start a new initiative to move the needle on one of two separate issues: high school education outcomes, or drug abuse in the community.\n",
    "\n",
    "Also unfortunately, that is the entirety of what you've been told. And the mayor just went on a lobbyist-funded fact-finding trip in the Bahamas. In the meantime, you got your hands on two national datasets: one on SAT scores by state, and one on drug use by age. Start exploring these to look for useful patterns and possible hypotheses!\n",
    "\n",
    "---\n",
    "\n",
    "This project is focused on exploratory data analysis, aka \"EDA\". EDA is an essential part of the data science analysis pipeline. Failure to perform EDA before modeling is almost guaranteed to lead to bad models and faulty conclusions. What you do in this project are good practices for all projects going forward, especially those after this bootcamp!\n",
    "\n",
    "This lab includes a variety of plotting problems. Much of the plotting code will be left up to you to find either in the lecture notes, or if not there, online. There are massive amounts of code snippets either in documentation or sites like [Stack Overflow](https://stackoverflow.com/search?q=%5Bpython%5D+seaborn) that have almost certainly done what you are trying to do.\n",
    "\n",
    "**Get used to googling for code!** You will use it every single day as a data scientist, especially for visualization and plotting.\n",
    "\n",
    "#### Package imports"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    .-\"^`\\                                        /`^\"-.\n",
    "  .'   ___\\                                      /___   `.\n",
    " /    /.---.                                    .---.\\    \\\n",
    "|    //     '-.  ___________________________ .-'     \\\\    |\n",
    "|   ;|         \\/--------------------------//         |;   |\n",
    "\\   ||       |\\_)     Lim Kim Huat        (_/|       ||   /\n",
    " \\  | \\  . \\ ;                             || ; / .  / |  /\n",
    "  '\\_\\ \\\\ \\ \\ \\ |   GitHub : WingsMaker    ||/ / / // /_/'\n",
    "        \\\\ \\ \\ \\| Repo : dsi-sg-07_kimhuat |/ / / //\n",
    "         `'-\\_\\_\\project-02-v2-starter-code/_/_/-'`\n",
    "                '--------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import t\n",
    "from scipy.stats import scoreatpercentile\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "# this line tells jupyter notebook to put the plots in the notebook rather than saving them to file.\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# this line makes plots prettier on mac retina screens. If you don't have one it shouldn't do anything.\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1. Load the `sat_scores.csv` dataset and describe it\n",
    "\n",
    "---\n",
    "\n",
    "You should replace the placeholder path to the `sat_scores.csv` dataset below with your specific path to the file.\n",
    "\n",
    "### 1.1 Load the file with the `csv` module and put it in a Python dictionary\n",
    "\n",
    "The dictionary format for data will be the column names as key, and the data under each column as the values.\n",
    "\n",
    "Toy example:\n",
    "```python\n",
    "data = {\n",
    "    'column1':[0,1,2,3],\n",
    "    'column2':['a','b','c','d']\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dictionary(dictname):\n",
    "    # print it out for checking key by key\n",
    "    for dic in dictname:\n",
    "        print(f\"\\nkey = {dic} and items below:\")\n",
    "        print(dictname[dic])    \n",
    "        print(\"-\"*80)\n",
    "\n",
    "csv_scores = \"sat_scores.csv\"\n",
    "print(f\"\\nReading csv file {csv_scores} as dictionary\")\n",
    "try:\n",
    "    # open the csv file and convert to dictionary\n",
    "    dict_sat = pd.read_csv(csv_scores,na_filter=False,sep=',').to_dict()\n",
    "\n",
    "    # print it out for checking the SAT dictionary\n",
    "    print_dictionary(dict_sat)\n",
    "    \n",
    "except:\n",
    "    print(\"Unable to read \",csv_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make a pandas DataFrame object with the SAT dictionary, and another with the pandas `.read_csv()` function\n",
    "\n",
    "Compare the DataFrames using the `.dtypes` attribute in the DataFrame objects. What is the difference between loading from file and inputting this dictionary (if any)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe with the SAT dictionary\n",
    "df_from_dic = pd.DataFrame.from_dict(dict_sat)\n",
    "\n",
    "print(\"the data types of the dataframe from dictionary:\")\n",
    "print(df_from_dic.dtypes)\n",
    "print(\"\\nfirst 5 rows:\")\n",
    "print(df_from_dic.head())\n",
    "print(\"-\"*50)\n",
    "\n",
    "# make a dataframe from csv file, turn off the na_filter\n",
    "csv_scores = \"sat_scores.csv\"\n",
    "print(f\"\\nReading csv file {csv_scores}\")\n",
    "try:\n",
    "    # follow the same column name as previous dataframe df_from_dic\n",
    "    df_from_file = pd.read_csv(csv_scores,na_filter=False,names=df_from_dic.columns,skiprows=1,sep=',')\n",
    "except:\n",
    "    print(f\"something is wrong with reading the file {csv_scores}\")\n",
    "\n",
    "# list all unique values, check for abnormal values\n",
    "colname = \"State\"\n",
    "unique_col_vals = df_from_file[colname].unique()\n",
    "\n",
    "# And sort by first column since it seems not major problems\n",
    "unique_col_vals = sorted(unique_col_vals)\n",
    "print(\"\\t\\t\",unique_col_vals)\n",
    "\n",
    "# sort the dataframe by State, preparing for the dictionary keys to be sorted\n",
    "df_from_file = df_from_file.sort_values(by='State')\n",
    "\n",
    "# df_from_file => convert to float\n",
    "df_from_file['Rate'] = df_from_file['Rate'].astype('float')\n",
    "df_from_file['Verbal'] = df_from_file['Verbal'].astype('float')\n",
    "df_from_file['Math'] = df_from_file['Math'].astype('float')\n",
    "\n",
    "print(\"\\nthe data types of the dataframe from file:\")\n",
    "print(df_from_file.dtypes)\n",
    "print(\"\\nfirst 5 rows:\")\n",
    "print(df_from_file.head())\n",
    "print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between loading from file and inputting this dictionary (if any)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " the difference betweem the two : \n",
    " Rate,Verbal,Math columns in were 'objects' in df_from_dic\n",
    " but were 'int64' in df_from_file\n",
    " and we changed the numberic columns from int64 to float64\n",
    "\"\"\"\n",
    "# compare columns with dtypes\n",
    "print(\"Comparing for columns for both dataframes:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"Columns\\tDictionary\\tRead CSV\")\n",
    "for x,v in enumerate(list(df_from_dic.dtypes)):\n",
    "    colname = df_from_dic.columns[x]\n",
    "    dictype = df_from_dic[colname].dtypes\n",
    "    csvtype = df_from_file[colname].dtypes\n",
    "    print(f\"{colname}\\t{dictype}\\t\\t{csvtype}\")\n",
    "\n",
    "print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not convert the string column values to float in your dictionary, the columns in the DataFrame are of type `object` (which are string values, essentially). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Look at the first ten rows of the DataFrame: what does our data describe?\n",
    "\n",
    "From now on, use the DataFrame loaded from the file using the `.read_csv()` function.\n",
    "\n",
    "Use the `.head(num)` built-in DataFrame function, where `num` is the number of rows to print out.\n",
    "\n",
    "You are not given a \"codebook\" with this data, so you will have to make some (very minor) inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data,remove the row with invalid \n",
    "df_sat = df_from_file[ df_from_file.State.str.len()==2].copy() # make a copy\n",
    "df_sat.dropna(how='all')                                       # remove NA's\n",
    "df_sat.reset_index(inplace=True)                               # reset index\n",
    "df_sat.drop(columns='index', inplace=True)                     # drop index columns\n",
    "df_sat.head()                                                  # first 5 rows\n",
    "\n",
    "# free up memory\n",
    "del df_from_dic, df_from_file                                  # free up variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2. Create a \"data dictionary\" based on the data\n",
    "\n",
    "---\n",
    "\n",
    "A data dictionary is an object that describes your data. This should contain the name of each variable (column), the type of the variable, your description of what the variable is, and the shape (rows and columns) of the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Codebook which describes the data:**\n",
    "______________________________________________________________________________ \n",
    "  - State (object type): the name of a state in the 2 letters format\n",
    "  - Rate  (float64 type) : the rate of people attending SAT test in a state\n",
    "  - Verbal (float64 type): the average verbal score of a state\n",
    "  - Math (float64 type)   : the average math score of a state\n",
    "  \n",
    "Rate, Verbal, Math should be float for decimal points when aggregating.\n",
    "\n",
    "______________________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking the SAT dataframe:\")\n",
    "print(\"\\n# of rows and columns\")\n",
    "print(df_sat.shape)\n",
    "print(\"\\nList out columns info\")\n",
    "print(df_sat.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. Plot the data using seaborn\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Using seaborn's `distplot`, plot the distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Set the keyword argument `kde=False`. This way you can actually see the counts within bins. You can adjust the number of bins to your liking. \n",
    "\n",
    "[Please read over the `distplot` documentation to learn about the arguments and fine-tune your chart if you want.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.distplot.html#seaborn.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the figure on top of each bar using ax.text()\n",
    "# ax is the axis object from plot object\n",
    "def AddDataLabel(ax):\n",
    "    for p in ax.patches:\n",
    "        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), \n",
    "                fontsize=12, color='black', ha='center', va='bottom')\n",
    "    return\n",
    "\n",
    "# Plot a seabon distribution bar graph\n",
    "# input :\n",
    "#        \"data\" - data series\n",
    "#        \"title\" - Title of the graph\n",
    "#        \"labelx\" - Label of the X axis\n",
    "#        \"labely\" - Label of the Y axis\n",
    "def SeabornDistPlot(data,title,labelx,labely):\n",
    "    sns.set(color_codes=True)   # turn on colour codes\n",
    "    subplt = plt.subplot(111)   # 1x1 grid and subplot #1\n",
    "    ax = sns.distplot(data, rug = True, bins =10, \n",
    "                      kde=False, ax=subplt, vertical=False)\n",
    "    ax.legend(loc='upper center')\n",
    "    ax.set_xlabel(labelx)\n",
    "    ax.set_ylabel(labely)\n",
    "    ax.set_title(title)\n",
    "    AddDataLabel(ax)\n",
    "    # Unable to fix below warning\n",
    "    # No handles with labels found to put in legend.\n",
    "    plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a graph , data take from \"Rate\" column of the dataframe\n",
    "SeabornDistPlot(df_sat.Rate, \"Rate of SAT test attended in a state\", \n",
    "                \"Index\",'# of peoples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a graph , data take from \"Verbal\" column of the dataframe\n",
    "SeabornDistPlot(df_sat.Verbal, \"Average verbal score of a state\", \n",
    "                \"Index\",'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a graph , data take from \"Math\" column of the dataframe\n",
    "SeabornDistPlot(df_sat.Math, \"Average math score of a state\", \n",
    "                \"Index\",'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using seaborn's `pairplot`, show the joint distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Explain what the visualization tells you about your data.\n",
    "\n",
    "[Please read over the `pairplot` documentation to fine-tune your chart.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html#seaborn.pairplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a seabon pair-plot graph\n",
    "# input :\n",
    "#        \"data\" - data series\n",
    "#        \"title\" - Title of the graph\n",
    "#        \"labelx\" - Label of the X axis\n",
    "#        \"labely\" - Label of the Y axis\n",
    "def SeabornPairPlot(data,title,labelx,labely):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.pairplot(data)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot from entire dataframe for SAT\n",
    "SeabornPairPlot(df_sat, \"Rate of SAT test attended in a state\", \"Index\",'# of peoples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 4. Plot the data using built-in pandas functions.\n",
    "\n",
    "---\n",
    "\n",
    "Pandas is very powerful and contains a variety of nice, built-in plotting functions for your data. Read the documentation here to understand the capabilities:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "### 4.1 Plot a stacked histogram with `Verbal` and `Math` using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a stacked histogram \n",
    "# input :\n",
    "#        \"data\" -   data series of 2 columns\n",
    "#        \"title\" -  Title of the graph\n",
    "#        \"labelx\" - Label of the X axis\n",
    "#        \"labely\" - Label of the Y axis\n",
    "def StackedHistogram(data,title,labelx,labely):\n",
    "    fig = plt.figure(1, figsize=(12, 8))\n",
    "    subplt = plt.subplot(111)\n",
    "    colors = ['blue', 'orange', 'green']\n",
    "    data.plot(kind='bar', width=0.6, stacked=True,ax=subplt) \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(labelx,fontsize=16)\n",
    "    plt.ylabel(labely,fontsize=16)\n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a stacked histogram on the Verbal and Math columns\n",
    "df=df_sat[[\"Verbal\",\"Math\"]]\n",
    "StackedHistogram(df, \"Verbal and Math\", \"Index\",'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Plot `Verbal` and `Math` on the same chart using boxplots\n",
    "\n",
    "What are the benefits of using a boxplot as compared to a scatterplot or a histogram?\n",
    "\n",
    "What's wrong with plotting a box-plot of `Rate` on the same chart as `Math` and `Verbal`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a box-plot \n",
    "# input :\n",
    "#        \"data\" -   data series of 2 columns\n",
    "#        \"title\" -  Title of the graph\n",
    "#        \"labelx\" - Label of the X axis\n",
    "#        \"labely\" - Label of the Y axis\n",
    "def PltBoxPlot(data,title,labelx,labely):\n",
    "    data.plot.box()\n",
    "    plt.xlabel(labelx,fontsize=16)\n",
    "    plt.ylabel(labely,fontsize=16)\n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()    \n",
    "    \n",
    "# Plot a Seaborn Box-plot\n",
    "# input :\n",
    "#        \"data\" -   data series of 2 columns\n",
    "#        \"title\" -  Title of the graph\n",
    "#        \"labelx\" - Label of the X axis\n",
    "#        \"labely\" - Label of the Y axis\n",
    "def SeabornBoxPlot(data,orien,title,labelx,labely):\n",
    "    fig = plt.figure(1, figsize=(12, 8))\n",
    "    subplt = plt.subplot(111)\n",
    "    ax = sns.boxplot(data=data, orient=orien, fliersize=5, linewidth=3, notch=True, saturation=0.5, ax=subplt)    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(labelx,fontsize=16)\n",
    "    plt.ylabel(labely,fontsize=16)\n",
    "    plt.title(title, fontsize=25)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate is not a score, should not in this plot with others\n",
    "# however just put into one boxplot to see \n",
    "# Verbal and Maths very similar but Rate is way below\n",
    "df = df_sat.loc[:,['Verbal','Math', 'Rate']]\n",
    "PltBoxPlot(df, 'SAT Score Dataframe', 'Type of Score', 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbal and Maths very similar but Maths has long tail\n",
    "df = df_sat.loc[:,['Verbal','Math']]\n",
    "PltBoxPlot(df, 'SAT Score Dataframe', 'Type of Score', 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4.3 Plot `Verbal`, `Math`, and `Rate` appropriately on the same boxplot chart\n",
    "\n",
    "Think about how you might change the variables so that they would make sense on the same chart. Explain your rationale for the choices on the chart. You should strive to make the chart as intuitive as possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing all columns and re-create the boxplot\n",
    "# We calculate the mean and standard deviation\n",
    "# and creates a normalised dataframe df_new \n",
    "# then plot a seaborn boxplot to show the 3 columns\n",
    "df = df_sat[['Rate','Verbal', 'Math']]\n",
    "df_mean = df_sat.mean()\n",
    "df_std  = df_sat.std() \n",
    "df_new = (df - df_mean)/df_std\n",
    "# Plot a horizontal seaborn box-plot showing  Rate, Verbal, Math\n",
    "SeabornBoxPlot(df_new,'h','SAT Score Dataframe', 'Type of Score', 'Score')\n",
    "del df_new, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. Create and examine subsets of the data\n",
    "\n",
    "---\n",
    "\n",
    "For these questions you will practice **masking** in pandas. Masking uses conditional statements to select portions of your DataFrame (through boolean operations under the hood.)\n",
    "\n",
    "Remember the distinction between DataFrame indexing functions in pandas:\n",
    "\n",
    "    .iloc[row, col] : row and column are specified by index, which are integers\n",
    "    .loc[row, col]  : row and column are specified by string \"labels\" (boolean arrays are allowed; useful for rows)\n",
    "    .ix[row, col]   : row and column indexers can be a mix of labels and integer indices\n",
    "    \n",
    "For detailed reference and tutorial make sure to read over the pandas documentation:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 Find the list of states that have `Verbal` scores greater than the average of `Verbal` scores across states\n",
    "\n",
    "How many states are above the mean? What does this tell you about the distribution of `Verbal` scores?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df_sat.mean()            # mean value of the columns in SAT dataframe\n",
    "mean_verbal = float(df_mean[1])    # mean value on the Verbal column\n",
    "print(f\"Average of Verbal score across the states {mean_verbal}\\n\")\n",
    "\n",
    "print(\"List of states having Verbal scores > the average of Verbal scores:\")\n",
    "df_sat[['State','Verbal']][df_sat.Verbal > mean_verbal].sort_values(by=\"Verbal\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Find the list of states that have `Verbal` scores greater than the median of `Verbal` scores across states\n",
    "\n",
    "How does this compare to the list of states greater than the mean of `Verbal` scores? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median = df_sat.median()            # median value of the columns in SAT dataframe\n",
    "median_verbal = float(df_median[1])    # median value on the Verbal column\n",
    "print(f\"Median of Verbal score across the states {median_verbal}\\n\")\n",
    "\n",
    "print(\"List of states having Verbal scores > the average of Verbal scores:\")\n",
    "df_sat[['State','Verbal']][df_sat.Verbal > median_verbal].sort_values(by=\"Verbal\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create a column that is the difference between the `Verbal` and `Math` scores\n",
    "\n",
    "Specifically, this should be `Verbal - Math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat[\"Verbal - Math\"] = (df_sat.Verbal - df_sat.Math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Create two new DataFrames showing states with the greatest difference between scores\n",
    "\n",
    "1. Your first DataFrame should be the 10 states with the greatest gap between `Verbal` and `Math` scores where `Verbal` is greater than `Math`. It should be sorted appropriately to show the ranking of states.\n",
    "2. Your second DataFrame will be the inverse: states with the greatest gap between `Verbal` and `Math` such that `Math` is greater than `Verbal`. Again, this should be sorted appropriately to show rank.\n",
    "3. Print the header of both variables, only showing the top 3 states in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat[df_sat.Verbal > df_sat.Math].sort_values(by=\"Verbal - Math\",ascending=True).head(10).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine summary statistics\n",
    "\n",
    "---\n",
    "\n",
    "Checking the summary statistics for data is an essential step in the EDA process!\n",
    "\n",
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.1 Create the correlation matrix of your variables (excluding `State`).\n",
    "\n",
    "What does the correlation matrix tell you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Verbal - Math\" column not needed for the correlation matrix\n",
    "if df_sat.shape[1]>4:\n",
    "    del df_sat[\"Verbal - Math\"]\n",
    "\n",
    "# find the correlation of the SAT dataframe\n",
    "print(\"The correlation matrix for SAT\")\n",
    "sat_corr = df_sat.corr()\n",
    "print(sat_corr)\n",
    "\n",
    "# Plot the heatmap graph\n",
    "sns.heatmap(sat_corr, annot=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Verbal & Math with correlation value 0.899909 as positive linear relationship\n",
    "Rate & Math with correlation value -0.773419 as negative linear relationship\n",
    "Verbal & Rate with correlation value -0.888121 as negative linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.2 Use pandas'  `.describe()` built-in function on your DataFrame\n",
    "\n",
    "Write up what each of the rows returned by the function indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "count  : there were 51 rows of records in the SAT Score\n",
    "mean   : average value of each columns\n",
    "std    : standard deviation of each columns\n",
    "min    : minimum value of each columns\n",
    "25%    : 25 percentile\n",
    "50%    : 50 percentile\n",
    "75%    : 75 percentile\n",
    "max    : maximum value of each columns\n",
    "\"\"\"\n",
    "df_sat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.3 Assign and print the _covariance_ matrix for the dataset\n",
    "\n",
    "1. Describe how the covariance matrix is different from the correlation matrix.\n",
    "2. What is the process to convert the covariance into the correlation?\n",
    "3. Why is the correlation matrix preferred to the covariance matrix for examining relationships in your data?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Describe how the covariance matrix is different from the correlation matrix.\n",
    "\n",
    "Covariance looks at the expected value of \"variations\" of two random variates from their expected values,” whereas correlation is the expected value of two random variates.\n",
    "\n",
    "Correlation values range from positive 1 to negative 1. On the other hand, covariance values can exceed this scale and unbounded."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is the process to convert the covariance into the correlation?\n",
    "\n",
    "Covariance divides by standard deviations of both random variables\n",
    "converts into Correlation (Pearson)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why is the correlation matrix preferred to the covariance matrix for examining relationships in your data?\n",
    "\n",
    "It takes combination of N*(N-1) to work out Covariance since \n",
    "we can only select 2 variables each time. However\n",
    "\n",
    "correlation takes in all the numeric variances in one take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 7. Performing EDA on \"drug use by age\" data.\n",
    "\n",
    "---\n",
    "\n",
    "You will now switch datasets to one with many more variables. This section of the project is more open-ended - use the techniques you practiced above!\n",
    "\n",
    "We'll work with the \"drug-use-by-age.csv\" data, sourced from and described here: https://github.com/fivethirtyeight/data/tree/master/drug-use-by-age.\n",
    "\n",
    "### 7.1\n",
    "\n",
    "Load the data using pandas. Does this data require cleaning? Are variables missing? How will this affect your approach to EDA on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_drug = \"drug-use-by-age.csv\"\n",
    "\n",
    "print(f\"\\nReading csv file {csv_drug}\")\n",
    "try:\n",
    "    df_drug = pd.read_csv(csv_drug,encoding='utf-8')\n",
    "except:\n",
    "    print(f\"something is wrong with reading the file {csv_scores}\")\n",
    "    df_drug = None\n",
    "if df_drug is not None:\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Look up for object data type\")\n",
    "print(df_drug.dtypes)\n",
    "\n",
    "print(\"\\nSome '-' among the  columns\")\n",
    "df_drug.select_dtypes(include=\"object\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for unused columns\n",
    "df_drug.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unused columns such as 'n' \n",
    "if df_drug.shape[1]>27:\n",
    "    del df_drug['n']\n",
    "\n",
    "# rename the columns\n",
    "# the frequency columns were actually median\n",
    "# in the null hypothesis, mean and median were the same\n",
    "# we rename those fields as mean for this case\n",
    "column_names = df_drug.columns\n",
    "column_names = [x.replace('-frequency','_mean') if ('-frequency' in x) else x for x in column_names]\n",
    "column_names = [x.replace('-','_') if ('-' in x) else x for x in column_names]\n",
    "print(column_names)\n",
    "df_drug.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age field containing range of values\n",
    "#df_drug['age'] = [float(x[0:2]) for x in df_drug.age]\n",
    "df_drug.age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign age group\n",
    "def agegroup(x):\n",
    "    # extract the age value\n",
    "    if len(x)==5 and x[2]=='-':\n",
    "        n=int(x[3:6])\n",
    "    elif len(x)==3 and x[2]=='+':        \n",
    "        n=int(x[0:2])\n",
    "    else:\n",
    "        n=int(x[0:2])\n",
    "\n",
    "    # Assign age-group \n",
    "    if n < 20:\n",
    "        return 1\n",
    "    elif n < 30:\n",
    "        return 2\n",
    "    elif n < 40:\n",
    "        return 3\n",
    "    elif n < 50:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "df_drug['age_group'] = df_drug['age'].apply(agegroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the results\n",
    "df_drug[['age','age_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These are the columns containing '-' inside:\")\n",
    "[col for col in df_drug.columns if [v for v in df_drug[col].unique() if v=='-'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make possible to assign '0' into a row/column during list comprehension\n",
    "def setvaluebyloc(colname, colindex):\n",
    "    df_drug[colname].iloc[colindex] = '0'\n",
    "    return f\"row #{colindex} in {colname} updated.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '-' with '0' for all 'object' field\n",
    "# and convert numeric string to float\n",
    "for col in list(df_drug.select_dtypes(include=\"object\").columns):    \n",
    "    if col==\"age\":\n",
    "        continue\n",
    "    [setvaluebyloc(col, k) for k in df_drug[col][df_drug[col].str.match('-')].keys() ]\n",
    "    df_drug[col]=df_drug[col].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The _use and _mean columns became float\n",
    "df_drug.info()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Do a high-level, initial overview of the data\n",
    "\n",
    "Get a feel for what this dataset is all about.\n",
    "\n",
    "Use whichever techniques you'd like, including those from the SAT dataset EDA. The final response to this question should be a written description of what you infer about the dataset.\n",
    "\n",
    "Some things to consider doing:\n",
    "\n",
    "- Look for relationships between variables and subsets of those variables' values\n",
    "- Derive new features from the ones available to help your analysis\n",
    "- Visualize everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap on all '_use' columns  and find the correlation values\n",
    "drug_use = df_drug[[x for x in df_drug.columns if x.find('_use')>0]]\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "drug_use_corr = drug_use.corr()\n",
    "sns.heatmap(drug_use_corr, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the highest absolute correlation and corresponding pair\n",
    "corrlen   = len(drug_use_corr)\n",
    "corrmax   = max([max([0 if x==1 else np.abs(x) for x in drug_use_corr[r]]) for r in drug_use_corr])\n",
    "drugspair = [drug for drug in [drug_use_corr.columns[n] if (corrmax in [np.abs(x) for x in drug_use_corr.iloc[n] if x<1]) else '' for n in range(corrlen)] if len(drug)>0]\n",
    "print(f\"Which 2 variables has the highest absolute correlation of:\\nvalue = {corrmax}:\\nwere : {drugspair}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the stacked bar graph of the drug percentage use by age group\n",
    "drug_use_df = df_drug[[x for x in df_drug.columns if (x=='age_group')|(x.find('_use')>0)]]\n",
    "drug_use_df.plot(x= 'age_group', kind = 'bar', stacked =True,figsize=(40,28))\n",
    "plt.xlabel('Age Group', fontsize = 35)\n",
    "plt.ylabel('Percentage of people using drug',fontsize = 35)\n",
    "plt.title = \"Drug use disribution by Age-Group\"\n",
    "plt.legend(loc='upper left',fontsize=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the heatmap to display the correlation between the meadian number of times a users in an age group\n",
    "drug_mean = df_drug[[x for x in df_drug.columns if x.find('_mean')>0]]\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "drug_mean_corr = drug_mean.corr()\n",
    "sns.heatmap(drug_mean_corr, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the highest absolute correlation and corresponding pair\n",
    "corrlen   = len(drug_mean_corr)\n",
    "corrmax   = max([max([0 if x==1 else np.abs(x) for x in drug_mean_corr[r]]) for r in drug_mean_corr])\n",
    "drugspair = [drug for drug in [drug_mean_corr.columns[n] if (corrmax in [np.abs(x) for x in drug_mean_corr.iloc[n] if x<1]) else '' for n in range(corrlen)] if len(drug)>0]\n",
    "print(f\"Which 2 variables has the highest absolute correlation of:\\nvalue = {corrmax}:\\nwere : {drugspair}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Findings:\n",
    "    cocaine and crack has the highest absolute correlation of 0.8722599764116105\n",
    "    Both were actually the same drugs , will be bias to set a hypothesis on it.\n",
    "Conclusion:\n",
    "    Find the 2nd highest absolute correlation.\n",
    "Fact:\n",
    "    https://www.therecoveryvillage.com/cocaine-addiction/cocaine-and-crack/#gref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the next highest absolute correlation and corresponding pair\n",
    "corrmax2  = max([max([0 if x>=corrmax else np.abs(x) for x in drug_mean_corr[r]]) for r in drug_mean_corr])\n",
    "drugspair = [drug for drug in [drug_mean_corr.columns[n] if (corrmax2 in [np.abs(x) for x in drug_mean_corr.iloc[n] if x<1]) else '' for n in range(corrlen)] if len(drug)>0]\n",
    "print(f\"Which 2 variables has the next highest absolute correlation of:\\nvalue = {corrmax2}:\\nwere : {drugspair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work out a hypothese over hallucinogen and inhalant instead of cocaine and crack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Create a testable hypothesis about this data\n",
    "\n",
    "Requirements for the question:\n",
    "\n",
    "1. Write a specific question you would like to answer with the data (that can be accomplished with EDA).\n",
    "2. Write a description of the \"deliverables\": what will you report after testing/examining your hypothesis?\n",
    "3. Use EDA techniques of your choice, numeric and/or visual, to look into your question.\n",
    "4. Write up your report on what you have found regarding the hypothesis about the data you came up with.\n",
    "\n",
    "\n",
    "Your hypothesis could be on:\n",
    "\n",
    "- Difference of group means\n",
    "- Correlations between variables\n",
    "- Anything else you think is interesting, testable, and meaningful!\n",
    "\n",
    "**Important notes:**\n",
    "\n",
    "You should be only doing EDA _relevant to your question_ here. It is easy to go down rabbit holes trying to look at every facet of your data, and so we want you to get in the practice of specifying a hypothesis you are interested in first and scoping your work to specifically answer that question.\n",
    "\n",
    "Some of you may want to jump ahead to \"modeling\" data to answer your question. This is a topic addressed in the next project and **you should not do this for this project.** We specifically want you to not do modeling to emphasize the importance of performing EDA _before_ you jump to statistical analysis."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.3.1\n",
    "Question:\n",
    "Is there any correlation between the frequency of the hallucinogen use and the inhalant use ? \n",
    "From the correlation matrix in 7.1, the correlation value was 0.88 !\n",
    "\n",
    "Goal:\n",
    "To find the answer, we define the H0 Null Hypothesis :\n",
    "[A]\n",
    "the difference of group means is insignificant over hallucinogen mean and inhalant mean\n",
    "[B]\n",
    "the correlation between frequency of hallucinogen use and inhalant use is insignificant.\n",
    "[C]\n",
    "Then use T statistics test to find the P value in order the conclude the hypothese."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.3.2\n",
    "Deliverables : \n",
    "[A]\n",
    "95% and 99% confidence interval for the median\n",
    "[B]\n",
    "Pearson correlation coefficient\n",
    "[C]\n",
    "Use boottraping technique to find the confidental internval\n",
    "P value as an indicator to accept or reject the hypothesis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.3.3\n",
    "EDA techniques with steps below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a line graph to see both figures\n",
    "hallucinogen = df_drug.hallucinogen_mean\n",
    "inhalant = df_drug.inhalant_mean\n",
    "cnt=df_drug['hallucinogen_mean'].count()\n",
    "x = np.arange(cnt)\n",
    "fig = plt.figure(1, figsize=(12, 8))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(x, df_drug['hallucinogen_mean'], label='hallucinogen_')\n",
    "ax.plot(x, df_drug['inhalant_mean'], label='inhalant')\n",
    "ax.set_title(\"Hallucinogen vs Inhalant\", fontsize=35)\n",
    "ax.set_xlabel(\"Record Number\", fontsize=25)\n",
    "ax.set_ylabel(\"Mean value\", fontsize=25)\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the pearson correlation coefficient between frequency of hallucinogen use and heroin use\n",
    "r = stats.pearsonr(hallucinogen, inhalant)[0]\n",
    "print(r, corrmax2)\n",
    "\n",
    "# it matches the value in the matrix from pearson perpecive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 random from the given data 'x'\n",
    "def bootstrap_r(x, y, iterations=1000):\n",
    "    boot_r = []\n",
    "    inds = list(range(len(x)))\n",
    "    for i in range(iterations):\n",
    "        boot_inds = np.random.choice(inds, replace=True, size=len(inds))\n",
    "        x_b = x[boot_inds]\n",
    "        y_b = y[boot_inds]\n",
    "        boot_r.append(stats.pearsonr(x_b, y_b)[0])\n",
    "    return boot_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_boots = bootstrap_r(hallucinogen, inhalant)\n",
    "\n",
    "# 95%\n",
    "lower = stats.scoreatpercentile(r_boots, 2.5)\n",
    "upper = stats.scoreatpercentile(r_boots, 97.5)\n",
    "\n",
    "print(lower, r, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_boots = bootstrap_r(hallucinogen, inhalant)\n",
    "\n",
    "# 99%\n",
    "lower = stats.scoreatpercentile(r_boots, 0.5)\n",
    "upper = stats.scoreatpercentile(r_boots, 99.5)\n",
    "\n",
    "print(lower, r, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a Seaborn Distplot based on bootstrap\n",
    "ax=sns.distplot(r_boots)\n",
    "ax.set_xlabel(\"Percentile\", fontsize=25)\n",
    "ax.set_ylabel(\"Score\", fontsize=25)\n",
    "\n",
    "ax.set_title(\"Hallucinogen & Inhalant Percentile Score\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using T-statistics to compute P-Value\n",
    "# Make a decision to accept or reject Null Hypothesis\n",
    "alpha = 0.05\n",
    "\n",
    "boot_mean = np.mean(r_boots)\n",
    "boot_median = np.median(r_boots)\n",
    "\n",
    "t_statistic = (boot_mean - boot_median)/(np.std(r_boots, ddof=1)/len(r_boots)**0.5)\n",
    "\n",
    "## Find p-value.\n",
    "p_value = t.sf(np.abs(t_statistic), len(r_boots))\n",
    "print(f\"P value is {p_value}\")\n",
    "\n",
    "if p_value < 0.5:\n",
    "    print(\"Conclusion : Null Hypothesis H0 can be rejected.\")\n",
    "else:\n",
    "    print(\"Conclusion : Null Hypothesis H0 can be accepted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_drug[['age','hallucinogen_mean','inhalant_mean']]\n",
    "ax=df1.plot(x= 'age', kind = 'bar', stacked =False,figsize=(16,10))\n",
    "plt.xlabel('Age', fontsize = 20)\n",
    "plt.ylabel('Median number of people using alcohol and inhalant',fontsize = 20)\n",
    "ax.set_title(\"Hallucinogen & Inhalant by Age\",fontsize=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**\n",
    "\n",
    "95% of the correlation coefficient values fall into the range \n",
    "from \n",
    "    0.5932094733684674  \n",
    "to  \n",
    "    0.9196997408994209\n",
    "\n",
    "99% of the correlation coeeficient values fall into the range \n",
    "from \n",
    "    0.33617842128346825 \n",
    "to  \n",
    "    0.9370982539051154\n",
    "\n",
    "As the P-value 1.0978138352132973e-14 < 0.5, it can be concluded that the correlation between the frequency of the hallucinogen use and the heroin use is significant regardless the age group. Reject H0 Null Hyphothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 8. Introduction to dealing with outliers\n",
    "\n",
    "---\n",
    "\n",
    "Outliers are an interesting problem in statistics, in that there is not an agreed upon best way to define them. Subjectivity in selecting and analyzing data is a problem that will recur throughout the course.\n",
    "\n",
    "1. Pull out the rate variable from the sat dataset.\n",
    "2. Are there outliers in the dataset? Define, in words, how you _numerically define outliers._\n",
    "3. Print out the outliers in the dataset.\n",
    "4. Remove the outliers from the dataset.\n",
    "5. Compare the mean, median, and standard deviation of the \"cleaned\" data without outliers to the original. What is different about them and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rate = df_sat[['Rate']]\n",
    "df_rate.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the statistical values on the Rate data\n",
    "rate_desc=df_rate.describe().T\n",
    "rate_max=rate_desc['max'][0]\n",
    "rate_min=rate_desc['min'][0]\n",
    "rate_avg=rate_desc['mean'][0]\n",
    "rate_std=rate_desc['std'][0]\n",
    "\n",
    "rate_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop/delete rows via record number\n",
    "def del_rows(df, df_filter):\n",
    "    df.drop([k for k in df_filter.keys()],inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An outlier is something that is above or below 1.5x std from the mean\n",
    "\n",
    "rate_outlier_high = rate_avg + rate_std * 1.5\n",
    "rate_outlier_low = rate_avg - rate_std  * 1.5\n",
    "\n",
    "print(f\"upper bound = {rate_outlier_high}, lower bound = {rate_outlier_low}\\n\")\n",
    "print(\"Outlier data point if exceed the upper bound from mean:\")\n",
    "if rate_max >= rate_outlier_high:\n",
    "    print(\"Found as following and will be removed:\")\n",
    "    df1 = df_rate['Rate'][df_rate['Rate'] >= rate_outlier_high]\n",
    "    print(df1)\n",
    "    # remove them\n",
    "    del_rows(df_rate, df1)\n",
    "else:\n",
    "    print(\"does not exists\")\n",
    "    \n",
    "print(\"Outlier data point if exceed the lower bound from mean:\")\n",
    "if rate_min <= rate_outlier_low:\n",
    "    print(\"Found as following and will be removed:\")\n",
    "    df1 = df_rate['Rate'][df_rate['Rate'] <= rate_outlier_low]\n",
    "    print(df1)\n",
    "    # remove them\n",
    "    del_rows(df_rate, df1)\n",
    "else:\n",
    "    print(\"does not exists \")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a seabon distplot graph\n",
    "# input :\n",
    "#        \"data\" - data series\n",
    "#        \"title\" - Title of the graph\n",
    "#        \"labelx\" - Label of the X axis\n",
    "#        \"labely\" - Label of the Y axis\n",
    "def univariate_analysis(data, title):\n",
    "    print('mean: {}'.format(np.mean(data)))\n",
    "    print('median: {}'.format(np.median(data)))\n",
    "    print('std: {}'.format(np.std(data)))\n",
    "    print('Percentiles:')\n",
    "    for p in range(0,100+1,10):\n",
    "        print('{}) {}'.format(p, np.percentile(data, p)))\n",
    "        \n",
    "    data = data - np.mean(data)\n",
    "    \n",
    "    print('Percentiles (after centering):')\n",
    "    for p in range(0,100+1,10):\n",
    "        print('{}) {}'.format(p, np.percentile(data, p)))\n",
    "        \n",
    "    sns.distplot(data)\n",
    "    ax.set_title(title, fontsize=35)\n",
    "    ax.set_xlabel(\"drugs values\", fontsize=25)\n",
    "    ax.set_ylabel(\"deviation\", fontsize=25)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_analysis(df_sat.Rate,\"Original Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_analysis(df_rate.Rate, \"Cleaned Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_original = df_sat.Rate.mean()\n",
    "median_original = df_sat.Rate.median()\n",
    "std_original = df_sat.Rate.std()\n",
    "\n",
    "mean_clean = df_rate.mean()\n",
    "median_clean = df_rate.median()\n",
    "std_clean = df_rate.std()[0]\n",
    "\n",
    "print(\"\\toriginal data \\t\\tvs cleaned data \")\n",
    "print(f\"mean : {mean_original} \\t\\t\\tvs {mean_clean[0]}\")\n",
    "print(f\"median : {median_original} \\t\\t\\tvs {median_clean[0]}\")\n",
    "print(f\"std : {std_original} \\tvs {std_clean}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Observation:\n",
    "mean , median ,standard devation decreased due to outlier data points removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 9. Percentile scoring and spearman rank correlation\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 Calculate the spearman correlation of sat `Verbal` and `Math`\n",
    "\n",
    "1. How does the spearman correlation compare to the pearson correlation? \n",
    "2. Describe clearly in words the process of calculating the spearman rank correlation.\n",
    "  - Hint: the word \"rank\" is in the name of the process for a reason!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1.1\n",
    "Spearman is a non-parametric test and Pearson is a parametric one.\n",
    "\n",
    "Spearman is for ordinal kind of data like \"Ranking\" and \n",
    "Pearson is for ratio scale kind of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1.2\n",
    "\n",
    "Objective :\n",
    "Find the ranks in the two data and compute the Spearman rank correlation.\n",
    "\n",
    "Step 1: \n",
    "Find the ranks for each individual data, \n",
    "such as order the values from greatest to smallest.\n",
    "Assign the rank 1 to the highest score, 2 to the next highest and so on.\n",
    "\n",
    "Step 2: \n",
    "Add a new column to the data as the difference between ranks. \n",
    "Add another column, square the values from previous.\n",
    "\n",
    "Step 4: \n",
    "Sum all of the squared values from the last above mentioned column.\n",
    "\n",
    "Step 5: \n",
    "Insert the values into the formula. \n",
    "\n",
    "\n",
    "### $$ \\text{pearson correlation}\\;r = cor(X, Y) =\\frac{cov(X, Y)}{std(X)std(Y)}$$\n",
    "\n",
    "The direct Python code :\n",
    "\n",
    "    numpy.corrcoef(var1,var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Percentile scoring\n",
    "\n",
    "Look up percentile scoring of data. In other words, the conversion of numeric data to their equivalent percentile scores.\n",
    "\n",
    "http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\n",
    "\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.percentileofscore.html\n",
    "\n",
    "1. Convert `Rate` to percentiles in the sat scores as a new column.\n",
    "2. Show the percentile of California in `Rate`.\n",
    "3. How is percentile related to the spearman rank correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat['PCT'] = df_sat['Rate'].apply(lambda x: np.percentile(df_sat.Rate,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat.sort_values(\"PCT\").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The percentile for state='CA' is {list(df_sat[(df_sat.State == 'CA')].PCT)[0]}\\n\")\n",
    "print(df_sat[(df_sat.State == 'CA')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.2.3. How is percentile related to the spearman rank correlation?**\n",
    "\n",
    "The percentile score of a data point is equal to its rank divided by the number of data points. \n",
    "\n",
    "Below a graph with \"Ranking\" added, in compare with the percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat['Rank'] = df_sat['PCT'].rank(ascending=1)\n",
    "\n",
    "list_rank = list(df_sat['Rank'])\n",
    "list_pct = list(df_sat['PCT'])\n",
    "cnt_rank = len(list_rank)\n",
    "range_rank = [x for x in range(1,1+cnt_rank)]\n",
    "\n",
    "plt.plot(range_rank,list_rank, linewidth=2,c='red')\n",
    "plt.plot(range_rank,list_pct, linewidth=3,c='blue')\n",
    "plt.xlabel(\"Record Number\")\n",
    "plt.ylabel(\"Ranks & Percentile\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Percentiles and outliers\n",
    "\n",
    "1. Why might percentile scoring be useful for dealing with outliers?\n",
    "2. Plot the distribution of a variable of your choice from the drug use dataset.\n",
    "3. Plot the same variable but percentile scored.\n",
    "4. Describe the effect, visually, of coverting raw scores to percentile."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9.3.1\n",
    "Why might percentile scoring be useful for dealing with outliers?\n",
    "\n",
    "Averages(mean,median) are useful when we don’t expect outliers.\n",
    "As Outliers were extreme values that are outside the range of what is expected.\n",
    "\n",
    "With percentile scoring, data points that have the percentile score outside the range can be an outlier. This range is also known as the confidence interval."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9.3.2\n",
    "Plot the distribution of a variable of your choice from the drug use dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotDist(data, title, showpercentile):\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.gca()   \n",
    "    var_mean=np.mean(data)\n",
    "    var_median=np.median(data)\n",
    "    var_std=np.std(data)\n",
    "    data = data - np.mean(data)    \n",
    "    var_xtickers=[p for p in range(0,100+1,10)]\n",
    "    var_percentile=[np.percentile(data, p) for p in range(0,100+1,10)]\n",
    "    sns.distplot(data, kde=True, hist=False)\n",
    "    plt.xlabel(\"drugs values\",fontsize=25)\n",
    "    plt.ylabel(\"deviation\",fontsize=25)\n",
    "    if showpercentile==True:\n",
    "        for p in var_xtickers:\n",
    "            label_pct=f\"{p}%\"\n",
    "            pct=np.percentile(data, p)\n",
    "            ax.axvline(pct, ls='dashed', lw=3, color='#333333', alpha=0.7)\n",
    "    for p in ax.patches:\n",
    "        ax.text(p.get_x() + p.get_width()/2., p.get_height(), round(p.get_height(),2), \n",
    "                fontsize=15, color='black', ha='center', va='bottom')\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDist(df_drug.hallucinogen_use,\"Distribution of hallucinogen\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDist(df_drug.inhalant_use,\"Distribution of inhalant\",False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9.3.3\n",
    "Plot the same variable but percentile scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDist(df_drug.hallucinogen_use,\"Distribution of hallucinogen\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotDist(df_drug.inhalant_use,\"Distribution of inhalant\",True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9.3.4\n",
    "Describe the effect, visually, of coverting raw scores to percentile.\n",
    "\n",
    "The veritical dash lines were the percentile for 0,10,20,30,...,100 percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
